models to add:

- swinv2
- Slak
- ReplkNet
- MVit
- EfficientVit https://arxiv.org/pdf/2205.14756
- BiFormer
- mamba out (they say they report macs but seem to be reporting flops)
- groupmamba
- plain mamba
- Exploring Plain Vision Transformer Backbones

other data sources:
- Detrex (no flops)
- mmseg benchmarks (no flops)
- Timm benchmarks

left in papers
- swin
-- Sparse RCNN, ATSS OD
- deit iii
-- ViT-L and ViT-B finetuned at 384 UPerNet (e.g. using 384 finetuned model as pretrained model for upernet); cifar, and a couple other classification results
- hiera
-- inaturalist finetune results and places365;
- internimage
-- internimage-h (trained on joint semi-labeled datasets; downstream is with dino and mask2former)
- focalnet
-- focalnet-h COCO DINO
-- focalnet-L finetuned at 384 mask2former
-- Sparse RCNN, ATSS OD
- MetaFormers
-- ade20k FPS for swin, convnext, and metaformers (unclear what the precision is)
- VMamba
-- fps results for higher resolutions (not trained at higher res though)

leftover cross references
- efficientNetv2
-- deit iii paper has throughput on v100 amp, imagenetv2

- internimage
-- convnext-b fp16 a100 throughput(?)

- VMamba
-- swin, convnext, deit fps results a100 fp32 (I think)

- hiera
-- vit, mcmae a100 fp16 classificaiton speed 
-- mvit, vitdet v100 fp32 det speed

- MetaFormers
-- Swin and Convnext a100 TF32 throughput that differs from convnext report (is faster)

other notes
- CSwin
-- Need to confirm pub date and CMaskRNN FPS details (this also goes for cswin and swin upernet fps results)
- hiera
-- flop count
- MetaFormers
-- They say they use macs, but there numbers suggest they are using flops. I assume flops. Need to confirm.
-- Need to confirm precision of Downstream FPS measurements.

code
- add filter by # citations
- add filter by model type (CNN, ATT, etc.) and structure (ISO, HIER, etc.)
- add flip test info for semantic seg
